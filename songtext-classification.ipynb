{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7139090,"sourceType":"datasetVersion","datasetId":4120108},{"sourceId":7140651,"sourceType":"datasetVersion","datasetId":4121303},{"sourceId":7333151,"sourceType":"datasetVersion","datasetId":4257044},{"sourceId":7337391,"sourceType":"datasetVersion","datasetId":4259790},{"sourceId":7339695,"sourceType":"datasetVersion","datasetId":4261382},{"sourceId":7340322,"sourceType":"datasetVersion","datasetId":4261779}],"dockerImageVersionId":30579,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Load required libraries\n\n# Load required libraries\n\nimport pandas as pd\nimport numpy as np\nimport sklearn as sk\nimport nltk\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom imblearn.over_sampling import SMOTE\n\nimport re\nimport random\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import SnowballStemmer\nfrom sklearn.preprocessing import LabelEncoder\nfrom collections import defaultdict\nfrom nltk.corpus import wordnet as wn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import model_selection, naive_bayes, svm\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n\nfrom imblearn.under_sampling import RandomUnderSampler\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, DistilBertModel, AdamW\nfrom transformers import AutoTokenizer\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom torch.nn import Dropout\nfrom torch.nn import Linear\nfrom tqdm import tqdm\nimport torch.nn.functional as F\n\n\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport torch.nn.functional as F\nfrom transformers import DistilBertConfig\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\n\n\n\nfrom sklearn.metrics.pairwise import cosine_similarity","metadata":{"execution":{"iopub.status.busy":"2024-01-04T23:45:56.769901Z","iopub.execute_input":"2024-01-04T23:45:56.770263Z","iopub.status.idle":"2024-01-04T23:45:56.781315Z","shell.execute_reply.started":"2024-01-04T23:45:56.770237Z","shell.execute_reply":"2024-01-04T23:45:56.780148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport pandas as pd\nfrom sklearn.utils import resample\ndf_1 = pd.read_csv('/kaggle/input/equal-sample/equal_sample_english_songs_50000.csv')","metadata":{"execution":{"iopub.status.busy":"2024-01-04T23:45:57.143981Z","iopub.execute_input":"2024-01-04T23:45:57.144348Z","iopub.status.idle":"2024-01-04T23:45:58.056917Z","shell.execute_reply.started":"2024-01-04T23:45:57.144315Z","shell.execute_reply":"2024-01-04T23:45:58.056112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Sample 10000 random records from the dataset\ndf = resample(df_1, n_samples=10000, random_state=42, replace=False)\n\n# Get the counts of each tag/category\ntag_counts = df['tag'].value_counts()\n\nprint(tag_counts)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T23:45:58.153007Z","iopub.execute_input":"2024-01-04T23:45:58.153367Z","iopub.status.idle":"2024-01-04T23:45:58.172279Z","shell.execute_reply.started":"2024-01-04T23:45:58.153341Z","shell.execute_reply":"2024-01-04T23:45:58.171358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, DistilBertModel, AdamW\nfrom transformers import AutoTokenizer\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-01-04T23:45:58.616667Z","iopub.execute_input":"2024-01-04T23:45:58.617444Z","iopub.status.idle":"2024-01-04T23:45:58.624737Z","shell.execute_reply.started":"2024-01-04T23:45:58.617408Z","shell.execute_reply":"2024-01-04T23:45:58.623845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the dimensions of the DataFrame\nprint(df.shape)\n\n# Calculate the count of each value in the \"Trainings-Labels\" column\nlabel_counts = df['tag'].value_counts()\n\n# Print the label counts\nprint(label_counts)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T23:45:59.035720Z","iopub.execute_input":"2024-01-04T23:45:59.036101Z","iopub.status.idle":"2024-01-04T23:45:59.044044Z","shell.execute_reply.started":"2024-01-04T23:45:59.036076Z","shell.execute_reply":"2024-01-04T23:45:59.042992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_mapping = {'pop': 0, 'rap': 1, 'rock': 2, 'rb': 3, \"country\": 4}\n\nlyrics_list = df['lyrics'].tolist()\ntag_list = df['tag'].tolist()\n\nnumeric_labels = torch.tensor([label_mapping[label] for label in tag_list])\nnumeric_labels","metadata":{"execution":{"iopub.status.busy":"2024-01-04T23:45:59.308317Z","iopub.execute_input":"2024-01-04T23:45:59.308679Z","iopub.status.idle":"2024-01-04T23:45:59.324793Z","shell.execute_reply.started":"2024-01-04T23:45:59.308623Z","shell.execute_reply":"2024-01-04T23:45:59.323825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_counts = pd.Series(numeric_labels).value_counts()\nprint(label_counts)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T23:45:59.649106Z","iopub.execute_input":"2024-01-04T23:45:59.650155Z","iopub.status.idle":"2024-01-04T23:45:59.657494Z","shell.execute_reply.started":"2024-01-04T23:45:59.650093Z","shell.execute_reply":"2024-01-04T23:45:59.656450Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'distilbert-base-uncased'\ntokenizer = DistilBertTokenizer.from_pretrained(model_name)\nmodel = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=len(label_mapping))\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T23:46:00.019120Z","iopub.execute_input":"2024-01-04T23:46:00.020171Z","iopub.status.idle":"2024-01-04T23:46:01.054303Z","shell.execute_reply.started":"2024-01-04T23:46:00.020124Z","shell.execute_reply":"2024-01-04T23:46:01.053303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SongsDataset(Dataset):\n    def __init__(self, songs, labels, tokenizer):\n        self.songs = songs\n        self.labels = labels\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.songs)\n\n    def __getitem__(self, idx):\n        song = str(self.songs[idx])\n        label = self.labels[idx]\n\n        encoding = self.tokenizer.encode_plus(\n            song,\n            truncation=True,\n            padding='max_length',\n            max_length=512,\n            return_tensors='pt'\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].squeeze(),\n            'attention_mask': encoding['attention_mask'].squeeze(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2024-01-04T23:46:01.056527Z","iopub.execute_input":"2024-01-04T23:46:01.056945Z","iopub.status.idle":"2024-01-04T23:46:01.064873Z","shell.execute_reply.started":"2024-01-04T23:46:01.056906Z","shell.execute_reply":"2024-01-04T23:46:01.063955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k = 2# number of folds\n\nskf = StratifiedKFold(n_splits=k, random_state=42, shuffle=True)\n\n# Lists for saving the training and test data\ntrain_lyrics_list = []\ntest_lyrics_list = []\ntrain_labels_list = []\ntest_labels_list = []\n\n# K-Fold cross-validation for splitting the data\nfor train_index, test_index in skf.split(lyrics_list, numeric_labels):\n    train_lyrics = [lyrics_list[i] for i in train_index]\n    test_lyrics = [lyrics_list[i] for i in test_index]\n    train_labels = [numeric_labels[i].item() for i in train_index]\n    test_labels = [numeric_labels[i].item() for i in test_index]\n\n    train_lyrics_list.append(train_lyrics)\n    test_lyrics_list.append(test_lyrics)\n    train_labels_list.append(train_labels)\n    test_labels_list.append(test_labels)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T23:46:01.066066Z","iopub.execute_input":"2024-01-04T23:46:01.066554Z","iopub.status.idle":"2024-01-04T23:46:01.164610Z","shell.execute_reply.started":"2024-01-04T23:46:01.066527Z","shell.execute_reply":"2024-01-04T23:46:01.163614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create training and test records for each fold\ntrain_datasets = []\ntest_datasets = []\n\nfor fold in range(k):\n    train_dataset = SongsDataset(train_lyrics_list[fold], train_labels_list[fold], tokenizer)\n    test_dataset = SongsDataset(test_lyrics_list[fold], test_labels_list[fold], tokenizer)\n\n    train_datasets.append(train_dataset)\n    test_datasets.append(test_dataset)\n\n# Create training and test data loaders for each fold\ntrain_dataloaders = []\ntest_dataloaders = []\n\nfor fold in range(k):\n    train_dataloader = DataLoader(train_datasets[fold], batch_size=16, shuffle=True)\n    test_dataloader = DataLoader(test_datasets[fold], batch_size=16, shuffle=False)\n\n    train_dataloaders.append(train_dataloader)\n    test_dataloaders.append(test_dataloader)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T23:46:01.268536Z","iopub.execute_input":"2024-01-04T23:46:01.268902Z","iopub.status.idle":"2024-01-04T23:46:01.280319Z","shell.execute_reply.started":"2024-01-04T23:46:01.268874Z","shell.execute_reply":"2024-01-04T23:46:01.279259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the number of records in the training and test datasets\nfor fold in range(k):\n    print(f\"Fold {fold+1}\")\n    train_dataset = train_datasets[fold]\n    test_dataset = test_datasets[fold]\n\n    print(f\"Number of training data: {len(train_dataset)}\")\n    print(f\"Number of testing data: {len(test_dataset)}\")\n\n# Verify the number of batches in the training and test data loaders.\nfor fold in range(k):\n    print(f\"Fold {fold+1}\")\n    train_dataloader = train_dataloaders[fold]\n    test_dataloader = test_dataloaders[fold]\n\n    print(f\"Number of batches in the training dataloader: {len(train_dataloader)}\")\n    print(f\"Number of batches in the testing dataloader: {len(test_dataloader)}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T23:46:01.686944Z","iopub.execute_input":"2024-01-04T23:46:01.687291Z","iopub.status.idle":"2024-01-04T23:46:01.693890Z","shell.execute_reply.started":"2024-01-04T23:46:01.687262Z","shell.execute_reply":"2024-01-04T23:46:01.693026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert train_labels to a numpy array\ntrain_labels_array = np.array(train_labels)\n\n# Count the number of data series per numeric_label\nunique_labels, label_counts = np.unique(train_labels_array, return_counts=True)\n\n# Create a dictionary to represent the number of data series per numeric_label\nlabel_counts_dict = dict(zip(unique_labels, label_counts))\n\n# Output the number of data series per numeric_label\nfor label, count in label_counts_dict.items():\n    print(f\"Numeric_Label {label}: {count} Data series\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T23:46:02.125955Z","iopub.execute_input":"2024-01-04T23:46:02.126814Z","iopub.status.idle":"2024-01-04T23:46:02.134028Z","shell.execute_reply.started":"2024-01-04T23:46:02.126759Z","shell.execute_reply":"2024-01-04T23:46:02.132995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert test_labels to a numpy array\ntest_labels_array = np.array(test_labels)\n\n# Count the number of data series per numeric_label\nunique_labels, label_counts = np.unique(test_labels_array, return_counts=True)\n\n# Create a dictionary to represent the number of data series per numeric_label\nlabel_counts_dict = dict(zip(unique_labels, label_counts))\n\n# Output the number of data series per numeric_label\nfor label, count in label_counts_dict.items():\n    print(f\"Numeric_Label {label}: {count} Data series\")","metadata":{"execution":{"iopub.status.busy":"2024-01-04T23:46:02.557413Z","iopub.execute_input":"2024-01-04T23:46:02.557845Z","iopub.status.idle":"2024-01-04T23:46:02.566214Z","shell.execute_reply.started":"2024-01-04T23:46:02.557801Z","shell.execute_reply":"2024-01-04T23:46:02.565174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nnum_epochs = 8\nlearning_rate = 1e-4\nwarmup_steps = 200","metadata":{"execution":{"iopub.status.busy":"2024-01-04T23:55:15.894492Z","iopub.execute_input":"2024-01-04T23:55:15.895204Z","iopub.status.idle":"2024-01-04T23:55:15.899754Z","shell.execute_reply.started":"2024-01-04T23:55:15.895164Z","shell.execute_reply":"2024-01-04T23:55:15.898852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training for each fold\nfor fold in range(k):\n    print(f\"Fold {fold+1}/{k}\")\n\n    train_dataloader = train_dataloaders[fold]\n\n    model.to(device)\n    model.train()\n\n    optimizer = AdamW(model.parameters(), lr=learning_rate)\n    total_steps = len(train_dataloader) * num_epochs\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n\n    for epoch in range(num_epochs):\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n\n        total_train_loss = 0\n\n        for batch in tqdm(train_dataloader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            optimizer.zero_grad()\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n\n            loss = nn.CrossEntropyLoss()(logits, labels)\n            total_train_loss += loss.item()\n\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n            optimizer.step()\n            scheduler.step()\n\n        avg_train_loss = total_train_loss / len(train_dataloader)\n        print(f\"Train Loss: {avg_train_loss}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-04T23:55:18.063307Z","iopub.execute_input":"2024-01-04T23:55:18.063682Z","iopub.status.idle":"2024-01-05T00:28:59.653820Z","shell.execute_reply.started":"2024-01-04T23:55:18.063650Z","shell.execute_reply":"2024-01-05T00:28:59.652855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n# Evaluation for each fold\nfor fold in range(k):\n    print(f\"Evaluation for Fold {fold+1}/{k}\")\n\n    test_dataloader = test_dataloaders[fold]\n\n    model.eval()\n    total_eval_loss = 0\n    predictions = []\n    true_labels = []\n\n    for batch in tqdm(test_dataloader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n\n        with torch.no_grad():\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n\n        loss = nn.CrossEntropyLoss()(logits, labels)\n        total_eval_loss += loss.item()\n\n        logits = logits.detach().cpu().numpy()\n        label_ids = labels.to('cpu').numpy()\n\n        predictions.extend(np.argmax(logits, axis=1))\n        true_labels.extend(label_ids)\n\n    avg_eval_loss = total_eval_loss / len(test_dataloader)\n    print(f\"Evaluation Loss: {avg_eval_loss}\")\n\n    accuracy = accuracy_score(true_labels, predictions)\n    f1 = f1_score(true_labels, predictions, average='weighted')\n\n    print(f\"Accuracy: {accuracy}\")\n    print(f\"F1 Score: {f1}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-04T23:36:23.282594Z","iopub.execute_input":"2024-01-04T23:36:23.282983Z","iopub.status.idle":"2024-01-04T23:38:52.666601Z","shell.execute_reply.started":"2024-01-04T23:36:23.282946Z","shell.execute_reply":"2024-01-04T23:38:52.665578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classification Report for each fold\nfor fold in range(k):\n    print(f\"Classification Report for Fold {fold+1}/{k}\")\n\n    test_dataloader = test_dataloaders[fold]\n\n    model.eval()\n    total_eval_loss = 0\n    predictions = []\n    true_labels = []\n\n    for batch in tqdm(test_dataloader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n\n        with torch.no_grad():\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n\n        loss = nn.CrossEntropyLoss()(logits, labels)\n        total_eval_loss += loss.item()\n\n        logits = logits.detach().cpu().numpy()\n        label_ids = labels.to('cpu').numpy()\n\n        predictions.extend(np.argmax(logits, axis=1))\n        true_labels.extend(label_ids)\n\n    avg_eval_loss = total_eval_loss / len(test_dataloader)\n    print(f\"Evaluation Loss: {avg_eval_loss}\")\n\n    accuracy = accuracy_score(true_labels, predictions)\n    print(f\"Accuracy: {accuracy}\")\n\n    # Create the Classification Report\n    report = classification_report(true_labels, predictions)\n    print(\"Classification Report:\")\n    print(report)\n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-01-04T23:38:52.668239Z","iopub.execute_input":"2024-01-04T23:38:52.668626Z","iopub.status.idle":"2024-01-04T23:41:22.408623Z","shell.execute_reply.started":"2024-01-04T23:38:52.668589Z","shell.execute_reply":"2024-01-04T23:41:22.407699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the Confusion Matrix\nconfusion_mat = confusion_matrix(true_labels, predictions, labels=np.unique(true_labels))\n\n# Labels for the axes labels\nlabel_names = list(label_mapping.keys())\n\n# Convert the Confusion Matrix to a Pandas DataFrame\ncm_df = pd.DataFrame(confusion_mat, index=label_names, columns=label_names)\n\n# Plot of the Confusion Matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm_df, annot=True, cmap=\"Blues\", fmt=\"d\")\nplt.xlabel(\"Predicted Labels\")\nplt.ylabel(\"True Labels\")\nplt.title(\"Confusion Matrix\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T23:41:22.411746Z","iopub.execute_input":"2024-01-04T23:41:22.412637Z","iopub.status.idle":"2024-01-04T23:41:22.788704Z","shell.execute_reply.started":"2024-01-04T23:41:22.412595Z","shell.execute_reply":"2024-01-04T23:41:22.787760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#torch.save(model, 'llm_model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-01-04T01:51:59.578311Z","iopub.execute_input":"2024-01-04T01:51:59.578674Z","iopub.status.idle":"2024-01-04T01:52:00.007670Z","shell.execute_reply.started":"2024-01-04T01:51:59.578645Z","shell.execute_reply":"2024-01-04T01:52:00.006835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model = torch.load('/kaggle/input/modell/llm_model(1).pth')\n#model.to('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-01-04T23:41:22.789828Z","iopub.execute_input":"2024-01-04T23:41:22.790130Z","iopub.status.idle":"2024-01-04T23:41:22.794310Z","shell.execute_reply.started":"2024-01-04T23:41:22.790103Z","shell.execute_reply":"2024-01-04T23:41:22.793330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T23:41:22.795484Z","iopub.execute_input":"2024-01-04T23:41:22.795754Z","iopub.status.idle":"2024-01-04T23:41:22.805591Z","shell.execute_reply.started":"2024-01-04T23:41:22.795730Z","shell.execute_reply":"2024-01-04T23:41:22.804806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index_to_label = {index: label for label, index in label_mapping.items()}\n\nimport torch\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n\ndef prepare_text_for_prediction(text, tokenizer, max_length=512):\n    encoded_text = tokenizer.encode_plus(\n        text,\n        add_special_tokens=True,\n        max_length=max_length,\n        truncation=True,\n        padding='max_length',\n        return_tensors='pt'\n    )\n    return encoded_text\n\ndef predict_text(text, tokenizer, model, device,index_to_label):\n    # Prepare the text for prediction\n    inputs = prepare_text_for_prediction(text, tokenizer)\n\n    # Move the inputs to the correct device\n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs['attention_mask'].to(device)\n\n    # Predict using the model\n    model.to(device)\n    model.eval()\n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        softmax_scores = torch.nn.functional.softmax(logits, dim=1)\n        predicted_class_idx = torch.argmax(softmax_scores, dim=1).item()\n        predicted_label = index_to_label[predicted_class_idx]\n        probabilities = softmax_scores[0].tolist()\n\n    return predicted_label, probabilities\n\ntext = \"In the heart of the country, under a wide blue sky,Where the fields stretch out like a sweet lullaby.There's a story of love that never grows old,Of a cowboy's heart, brave and bold.Chorus:Whispers on the wind, calling me home,To the place where the wildflowers roam.In your arms, I find my peace,In the country's heart, my soul's release.Verse 2:Under the stars, by the light of the moon,We dance to the tune of an old country tune.With your hand in mine, troubles fade away,In each other's eyes, we find our way.Chorus:Whispers on the wind, calling me home,To the place where the rivers gently roam.In your arms, I find my peace,In the country's heart, love's sweet release.Bridge:Through the storms, through the pain,Our love's like a gentle rain.Washing away all the sorrow,Leading us to a bright tomorrow.Verse 3:As the sun sets, painting the sky,I thank the stars for you and I.In this simple life, we've found our dream,In the country's heart, we're a perfect team.Chorus:Whispers on the wind, calling me home,To the place where the old oaks stand strong.In your arms, I find my peace,In the country's heart, our love's sweet lease.Outro:So here we stand, in love's sweet embrace,In the country's heart, we've found our place.With whispers on the wind, and love so true,In the heart of the country, I found you.\"\npredicted_label, probabilities = predict_text(text, tokenizer, model, device, index_to_label)\nprint(f\"Predicted class: {predicted_label}, Predicted probabilities: {probabilities}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-04T23:41:22.806863Z","iopub.execute_input":"2024-01-04T23:41:22.807160Z","iopub.status.idle":"2024-01-04T23:41:22.866509Z","shell.execute_reply.started":"2024-01-04T23:41:22.807134Z","shell.execute_reply":"2024-01-04T23:41:22.865530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation of Final Test Data","metadata":{}},{"cell_type":"code","source":"df_finaltest = pd.read_csv('/kaggle/input/final-test/final_test_english_songs_10000.csv')","metadata":{"execution":{"iopub.status.busy":"2024-01-05T00:28:59.656043Z","iopub.execute_input":"2024-01-05T00:28:59.656726Z","iopub.status.idle":"2024-01-05T00:28:59.865301Z","shell.execute_reply.started":"2024-01-05T00:28:59.656679Z","shell.execute_reply":"2024-01-05T00:28:59.864382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"external_lyrics_list = df_finaltest['lyrics'].tolist()\nexternal_tag_list = df_finaltest['tag'].tolist()\n\nexternal_numeric_labels = torch.tensor([label_mapping[label] for label in external_tag_list])\n\n# Create an external dataset\nexternal_dataset = SongsDataset(external_lyrics_list, external_numeric_labels, tokenizer)\n\n# Create an external data loader\nexternal_dataloader = DataLoader(external_dataset, batch_size=16, shuffle=False)\n\n# Evaluation for the external dataset\nmodel.eval()\ntotal_external_loss = 0\nexternal_predictions = []\nexternal_true_labels = []\n\nfor batch in tqdm(external_dataloader):\n    input_ids = batch['input_ids'].to(device)\n    attention_mask = batch['attention_mask'].to(device)\n    labels = batch['label'].to(device)\n\n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n\n    loss = nn.CrossEntropyLoss()(logits, labels)\n    total_external_loss += loss.item()\n\n    logits = logits.detach().cpu().numpy()\n    label_ids = labels.to('cpu').numpy()\n\n    external_predictions.extend(np.argmax(logits, axis=1))\n    external_true_labels.extend(label_ids)\n\navg_external_loss = total_external_loss / len(external_dataloader)\nprint(f\"External Dataset Evaluation Loss: {avg_external_loss}\")\n\nexternal_accuracy = accuracy_score(external_true_labels, external_predictions)\nexternal_f1 = f1_score(external_true_labels, external_predictions, average='weighted')\n\nprint(f\"External Dataset Accuracy: {external_accuracy}\")\nprint(f\"External Dataset F1 Score: {external_f1}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-05T00:32:25.084006Z","iopub.execute_input":"2024-01-05T00:32:25.084388Z","iopub.status.idle":"2024-01-05T00:32:38.438906Z","shell.execute_reply.started":"2024-01-05T00:32:25.084358Z","shell.execute_reply":"2024-01-05T00:32:38.437306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the Confusion Matrix\nconfusion_mat = confusion_matrix(external_true_labels, external_predictions, labels=np.unique(external_true_labels))\n\n# Labels for the axes labels\nlabel_names = list(label_mapping.keys())\n\n# Convert the Confusion Matrix to a Pandas DataFrame\ncm_df = pd.DataFrame(confusion_mat, index=label_names, columns=label_names)\n\n# Plot of the Confusion Matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm_df, annot=True, cmap=\"Blues\", fmt=\"d\")\nplt.xlabel(\"Predicted Labels\")\nplt.ylabel(\"True Labels\")\nplt.title(\"Confusion Matrix\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-05T00:32:43.871768Z","iopub.execute_input":"2024-01-05T00:32:43.872619Z","iopub.status.idle":"2024-01-05T00:32:44.264858Z","shell.execute_reply.started":"2024-01-05T00:32:43.872583Z","shell.execute_reply":"2024-01-05T00:32:44.263983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# Classification Report for the external dataset\nprint(\"Classification Report for External Dataset\")\n\nmodel.eval()\ntotal_external_loss = 0\nexternal_predictions = []\nexternal_true_labels = []\n\nfor batch in tqdm(external_dataloader):\n    input_ids = batch['input_ids'].to(device)\n    attention_mask = batch['attention_mask'].to(device)\n    labels = batch['label'].to(device)\n\n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n\n    loss = nn.CrossEntropyLoss()(logits, labels)\n    total_external_loss += loss.item()\n\n    logits = logits.detach().cpu().numpy()\n    label_ids = labels.to('cpu').numpy()\n\n    external_predictions.extend(np.argmax(logits, axis=1))\n    external_true_labels.extend(label_ids)\n\navg_external_loss = total_external_loss / len(external_dataloader)\nprint(f\"External Dataset Evaluation Loss: {avg_external_loss}\")\n\nexternal_accuracy = accuracy_score(external_true_labels, external_predictions)\nprint(f\"External Dataset Accuracy: {external_accuracy}\")\n\n# Create the Classification Report for the external dataset\nexternal_report = classification_report(external_true_labels, external_predictions, target_names=label_mapping.keys())\nprint(\"Classification Report for External Dataset:\")\nprint(external_report)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-05T00:33:01.180375Z","iopub.execute_input":"2024-01-05T00:33:01.181663Z","iopub.status.idle":"2024-01-05T00:33:04.800574Z","shell.execute_reply.started":"2024-01-05T00:33:01.181611Z","shell.execute_reply":"2024-01-05T00:33:04.799241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}